# -*- coding: utf-8 -*-
"""DeepLearning98-97-97 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19MWhjXsA2RBm84fhFSKwutKW1zLzCinD

# Solving Median House Value classification problem implementing a **deep neural network with ReLU** and tf.Keras
"""

# Commented out IPython magic to ensure Python compatibility.
# Tensorflow and tf.keras
# %tensorflow_version 1.x
import tensorflow as tf
from tensorflow import keras

#Helper libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

# Google file system
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

ATT_FILE = "/gdrive/My Drive/DeepLearning/OceanProximityPreparedCleanAttributes.csv"
LABEL_FILE = "/gdrive/My Drive/DeepLearning/OceanProximityOneHotEncodedClasses.csv"
TRAIN_RATE=0.8

attributes = pd.read_csv(ATT_FILE)
label = pd.read_csv(LABEL_FILE)

n_instances = attributes.shape[0]
n_train = int(n_instances*TRAIN_RATE)
n_dev = int((n_instances-n_train)/2)
n_test = int((n_instances-n_train)/2)

x_train = attributes.values[:n_train]
t_train = label.values[:n_train]

x_dev = attributes.values[n_train:n_train+n_dev]
t_dev = label.values[n_train:n_train+n_dev]

x_test = attributes.values[n_train+n_dev:n_instances]
t_test = label.values[n_train+n_dev:n_instances]

print ("x_train:",x_train.shape)
print ("t_train:",t_train.shape)


print ("x_dev:",x_dev.shape)
print ("t_dev:",t_dev.shape)

print ("x_test:",x_test.shape)
print ("t_test:",t_test.shape)



"""## Initialization"""

INPUTS = x_train.shape[1]
OUTPUTS = t_train.shape[1]
NUM_TRAINING_EXAMPLES = int(round(x_train.shape[0]/1))
NUM_DEV_EXAMPLES = int (round (x_dev.shape[0]/1))

"""Some data is displayed to test correctness:"""

x_train[:5]

t_train[:10]

x_dev[:5]

t_dev[:5]

"""## Hyperparameters

The number of hidden layers and neurons per layer must be adjusted. In this example, we increase the computational power of the neural network by adding up to three hidden layers (to be deep) and more neurons per layer.
"""

n_epochs = 1500 # corresponding to about 20,000 iterations
learning_rate = 0.1
batch_size = 264
#n_neurons_per_hlayer = [2500,2500,2500]

"""## Build the deep neural model

First of all, a sequential model is created. This is the one of Keras models, for full-connected feedforward neural networks, in which layers are sequentially connected. This is called the *sequential* API.
"""

model = keras.Sequential(name="Feedforward NN")

"""Define the deep neural network topology. Note that the **tanh** activation function is chosen for the hidden layers and  **softmax** for the ouput layer. **We expect that the neural network does not train properly since we are employing an activation function that saturates at the edges**. 

Adding layers to the model. The model takes as input matrix tensors with *INPUTS* columns and *batch size* rows. *InputLayer* creates a placeholder where the data is ready to feed the network. Then the hidden layers with *tanh* activation function are created. Finally, the output layer with the *softmax* activation function is added.
"""

from keras.layers import  Dropout,Flatten, Dense,Activation
model.add(keras.layers.InputLayer( input_shape=(INPUTS,), batch_size=None))

model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dropout(0.015))
model.add(keras.layers.Dense(93, activation="relu"))
model.add(keras.layers.Dense(85, activation="relu"))









model.add(keras.layers.Dense(OUTPUTS, activation="softmax"))
model.summary()

"""For example, the kernel in the last layer comprises 303 parameters to adjust: 100 neurons in the previous layer by 3 neurons in the output layer plus the three biases, one for each output neuron."""

model.layers

for l in model.layers: print (l.name)

"""All the parameters of a layer can bee accessed:"""

weights, biases = model.layers[0].get_weights()
weights.shape

biases

biases.shape

"""# Compiling the model

Compiling the model means specifying the *loss* function (the $log-loss$,  $cross-entropy$, the sum of log-loss is a loss) and the *optimizer* (Gradient Descent) to use. Optionally, you can also specify a list of extra *metrics* (Accuracy) to compute during training and evaluation. In this case,
"""

model.compile(loss=tf.keras.losses.categorical_crossentropy,
              optimizer=tf.keras.optimizers.SGD(lr=learning_rate, nesterov=True),
              metrics=["categorical_accuracy"])

"""There are several losses functions, optimizers and metrics. Full lists are available at: https://keras.io/losses/, https://keras.io/optimizers/ and https://keras.io/metrics/.

## Training and validating the model with M-BGD

Note that an **epoch** is an iteration over the entire training dataset provided.
"""

import time
start = time.clock()
history = model.fit(x_train, t_train, batch_size=batch_size, epochs=n_epochs, verbose=2, validation_data=(x_dev, t_dev))    
print (time.clock() - start)

results=pd.DataFrame(history.history)
results.plot(figsize=(8, 5))
plt.grid(True)
plt.xlabel ("Epochs")
plt.ylabel ("Accuracy - Mean Log Loss")
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()

"""Note how the learning curves are plain. BP learning algorithm does not modify the weights of the connections because the derivative of the tanh fucntion is zero for high values of the net: **fundamental deep learning problem**."""

history.params

results[-1:]

print ("Accuracy for the training set: ", results.categorical_accuracy.values[-1:][0])

print ("Accuracy for the development test set: ", results.val_categorical_accuracy.values[-1:][0])



"""Now the accuracy is 73% on training and 73% on the develoment test set, very similar values to those achieved with just one hidden layer with 1,000 neurons.

Let's see how the model predicts on the development test set:
"""

dev_predictions=model.predict(x_dev).round(2)
dev_predictions[:20]

dev_rounded_predictions=np.round(dev_predictions)
indices = np.argmax(dev_predictions,1)
for row, index in zip(dev_rounded_predictions, indices): row[index]=1
dev_rounded_predictions[:20]

t_dev[:20] #target classes

dev_correct_predictions = np.equal(np.argmax(dev_rounded_predictions,1),np.argmax(t_dev,1))
print (dev_correct_predictions[:30])

from collections import Counter
Counter (dev_correct_predictions)

# evaluate the keras model
_,accuracy_2 = model.evaluate(x_test, t_test)
print('Accuracy: %.2f' % (accuracy_2*100))